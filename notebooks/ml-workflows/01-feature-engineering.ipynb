{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Feature Engineering Workflow\n",
        "\n",
        "**ğŸ“Š Category**: ML Workflow\n",
        "\n",
        "**ğŸ‘¤ Author**: Data Science Team\n",
        "\n",
        "**ğŸ“… Created**: 2024-01-15\n",
        "\n",
        "**ğŸ”„ Last Updated**: 2024-01-15\n",
        "\n",
        "**â±ï¸ Estimated Runtime**: 15-20 minutes\n",
        "\n",
        "**ğŸ¯ Purpose**: Demonstrate advanced feature engineering techniques including automated feature selection, transformation, and preprocessing for machine learning workflows.\n",
        "\n",
        "**ğŸ“‹ Prerequisites**: \n",
        "- Basic understanding of machine learning concepts\n",
        "- Familiarity with pandas and scikit-learn\n",
        "- Understanding of feature engineering principles\n",
        "\n",
        "**ğŸ“Š Datasets Used**:\n",
        "- Synthetic retail dataset: Generated dataset with sales, product, and customer features\n",
        "- Size: 10,000 rows, 15 features\n",
        "- Target: Sales prediction (regression) or customer category (classification)\n",
        "\n",
        "**ğŸ”§ Tools & Libraries**:\n",
        "- pandas: Data manipulation and analysis\n",
        "- scikit-learn: Machine learning algorithms and preprocessing\n",
        "- matplotlib/seaborn: Data visualization\n",
        "- numpy: Numerical computations\n",
        "- Custom ml_utils: Advanced ML utilities\n",
        "\n",
        "**ğŸ“ˆ Key Outcomes**:\n",
        "- Automated feature selection and engineering\n",
        "- Comprehensive preprocessing pipeline\n",
        "- Feature importance analysis\n",
        "- Optimized feature set for model training\n",
        "\n",
        "**ğŸ”— Related Notebooks**:\n",
        "- [02-model-training.ipynb](02-model-training.ipynb): Model training with engineered features\n",
        "- [03-model-evaluation.ipynb](03-model-evaluation.ipynb): Model evaluation and validation\n",
        "- [data-exploration-template.ipynb](../templates/data-exploration-template.ipynb): Data exploration template\n",
        "\n",
        "**ğŸ“ Change Log**:\n",
        "- v1.0.0 (2024-01-15): Initial implementation with automated feature engineering\n",
        "- v1.0.1 (2024-01-15): Added polynomial features and interaction terms\n",
        "- v1.0.2 (2024-01-15): Enhanced feature selection methods\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“š Table of Contents\n",
        "\n",
        "1. [Environment Setup](#environment-setup)\n",
        "2. [Data Loading and Exploration](#data-loading-exploration)\n",
        "3. [Feature Preprocessing](#feature-preprocessing)\n",
        "4. [Automated Feature Selection](#automated-feature-selection)\n",
        "5. [Feature Engineering](#feature-engineering)\n",
        "6. [Feature Validation](#feature-validation)\n",
        "7. [Pipeline Creation](#pipeline-creation)\n",
        "8. [Results Summary](#results-summary)\n",
        "9. [Next Steps](#next-steps)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "## 1. Environment Setup\n",
        "\n",
        "This section sets up the environment, imports necessary libraries, and configures the notebook for feature engineering workflows.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set up plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['axes.titlesize'] = 16\n",
        "plt.rcParams['axes.labelsize'] = 12\n",
        "\n",
        "# Machine learning libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, PolynomialFeatures\n",
        "from sklearn.feature_selection import SelectKBest, SelectFromModel, RFE, f_classif, f_regression\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
        "from sklearn.metrics import accuracy_score, r2_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Add shared utilities to path\n",
        "sys.path.append('../shared')\n",
        "\n",
        "# Import custom utilities\n",
        "try:\n",
        "    from ml_utils import FeatureEngineer, create_synthetic_dataset\n",
        "    from visualization_utils import create_subplot_grid, plot_feature_importance\n",
        "    from data_connectors import DataConnector\n",
        "    print(\"âœ… Successfully imported custom utilities\")\n",
        "except ImportError as e:\n",
        "    print(f\"âš ï¸  Warning: Could not import custom utilities: {e}\")\n",
        "    print(\"Using standard libraries only\")\n",
        "\n",
        "# Configuration\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "# Display configuration\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', 50)\n",
        "\n",
        "print(\"ğŸš€ Environment setup complete!\")\n",
        "print(f\"ğŸ“Š Pandas version: {pd.__version__}\")\n",
        "print(f\"ğŸ”¢ NumPy version: {np.__version__}\")\n",
        "print(f\"ğŸ“ˆ Matplotlib version: {plt.matplotlib.__version__}\")\n",
        "print(f\"ğŸ¯ Random state: {RANDOM_STATE}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "## 2. Data Loading and Exploration\n",
        "\n",
        "Load the dataset and perform initial exploration to understand the data structure, distributions, and relationships between features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create synthetic retail dataset for demonstration\n",
        "print(\"ğŸ“¦ Creating synthetic retail dataset...\")\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(RANDOM_STATE)\n",
        "n_samples = 10000\n",
        "\n",
        "# Create retail-specific features\n",
        "data = {\n",
        "    'customer_age': np.random.randint(18, 80, n_samples),\n",
        "    'annual_income': np.random.lognormal(10, 0.5, n_samples),\n",
        "    'purchase_frequency': np.random.poisson(12, n_samples),\n",
        "}\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "print(f\"âœ… Dataset created with {len(df)} samples and {len(df.columns)} features\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create synthetic retail dataset for demonstration\n",
        "print(\"ğŸ“¦ Creating synthetic retail dataset...\")\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(RANDOM_STATE)\n",
        "n_samples = 10000\n",
        "\n",
        "# Create retail-specific features\n",
        "data = {\n",
        "    'customer_age': np.random.randint(18, 80, n_samples),\n",
        "    'annual_income': np.random.lognormal(10, 0.5, n_samples),\n",
        "    'purchase_frequency': np.random.poisson(12, n_samples),\n",
        "    'avg_basket_size': np.random.exponential(50, n_samples),\n",
        "    'days_since_last_purchase': np.random.exponential(30, n_samples),\n",
        "    'loyalty_score': np.random.beta(2, 5, n_samples) * 100,\n",
        "    'product_category_electronics': np.random.binomial(1, 0.3, n_samples),\n",
        "    'product_category_clothing': np.random.binomial(1, 0.4, n_samples),\n",
        "    'product_category_food': np.random.binomial(1, 0.5, n_samples),\n",
        "    'seasonal_factor': np.sin(np.random.uniform(0, 2*np.pi, n_samples)) + 1,\n",
        "    'marketing_channel_online': np.random.binomial(1, 0.6, n_samples),\n",
        "    'marketing_channel_email': np.random.binomial(1, 0.4, n_samples),\n",
        "    'customer_segment': np.random.choice(['Premium', 'Standard', 'Budget'], n_samples, p=[0.2, 0.5, 0.3]),\n",
        "    'region': np.random.choice(['North', 'South', 'East', 'West'], n_samples),\n",
        "    'is_weekend': np.random.binomial(1, 0.3, n_samples)\n",
        "}\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Create target variable (sales amount) with realistic relationships\n",
        "target_sales = (\n",
        "    df['annual_income'] * 0.001 +\n",
        "    df['purchase_frequency'] * 5 +\n",
        "    df['avg_basket_size'] * 2 +\n",
        "    df['loyalty_score'] * 1.5 +\n",
        "    df['product_category_electronics'] * 50 +\n",
        "    df['seasonal_factor'] * 20 +\n",
        "    np.random.normal(0, 50, n_samples)  # Add noise\n",
        ")\n",
        "\n",
        "# Ensure positive values\n",
        "df['target_sales'] = np.maximum(target_sales, 0)\n",
        "\n",
        "# Create binary classification target\n",
        "df['target_high_value'] = (df['target_sales'] > df['target_sales'].quantile(0.7)).astype(int)\n",
        "\n",
        "print(f\"âœ… Dataset created with {len(df)} samples and {len(df.columns)} features\")\n",
        "print(f\"ğŸ“Š Target variable (sales) range: ${df['target_sales'].min():.2f} - ${df['target_sales'].max():.2f}\")\n",
        "print(f\"ğŸ¯ High-value customers: {df['target_high_value'].sum()} ({df['target_high_value'].mean()*100:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create synthetic retail dataset for demonstration\n",
        "print(\"ğŸ“¦ Creating synthetic retail dataset...\")\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(RANDOM_STATE)\n",
        "n_samples = 10000\n",
        "\n",
        "# Create retail-specific features\n",
        "data = {\n",
        "    'customer_age': np.random.randint(18, 80, n_samples),\n",
        "    'annual_income': np.random.lognormal(10, 0.5, n_samples),\n",
        "    'purchase_frequency': np.random.poisson(12, n_samples),\n",
        "    'avg_basket_size': np.random.exponential(50, n_samples),\n",
        "    'days_since_last_purchase': np.random.exponential(30, n_samples),\n",
        "    'loyalty_score': np.random.beta(2, 5, n_samples) * 100,\n",
        "    'product_category_electronics': np.random.binomial(1, 0.3, n_samples),\n",
        "    'product_category_clothing': np.random.binomial(1, 0.4, n_samples),\n",
        "    'product_category_food': np.random.binomial(1, 0.5, n_samples),\n",
        "    'seasonal_factor': np.sin(np.random.uniform(0, 2*np.pi, n_samples)) + 1,\n",
        "    'marketing_channel_online': np.random.binomial(1, 0.6, n_samples),\n",
        "    'marketing_channel_email': np.random.binomial(1, 0.4, n_samples),\n",
        "    'customer_segment': np.random.choice(['Premium', 'Standard', 'Budget'], n_samples, p=[0.2, 0.5, 0.3]),\n",
        "    'region': np.random.choice(['North', 'South', 'East', 'West'], n_samples),\n",
        "    'is_weekend': np.random.binomial(1, 0.3, n_samples)\n",
        "}\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Create target variable (sales amount) with realistic relationships\n",
        "target_sales = (\n",
        "    df['annual_income'] * 0.001 +\n",
        "    df['purchase_frequency'] * 5 +\n",
        "    df['avg_basket_size'] * 2 +\n",
        "    df['loyalty_score'] * 1.5 +\n",
        "    df['product_category_electronics'] * 50 +\n",
        "    df['seasonal_factor'] * 20 +\n",
        "    np.random.normal(0, 50, n_samples)  # Add noise\n",
        ")\n",
        "\n",
        "# Ensure positive values\n",
        "df['target_sales'] = np.maximum(target_sales, 0)\n",
        "\n",
        "# Create binary classification target\n",
        "df['target_high_value'] = (df['target_sales'] > df['target_sales'].quantile(0.7)).astype(int)\n",
        "\n",
        "print(f\"âœ… Dataset created with {len(df)} samples and {len(df.columns)} features\")\n",
        "print(f\"ğŸ“Š Target variable (sales) range: ${df['target_sales'].min():.2f} - ${df['target_sales'].max():.2f}\")\n",
        "print(f\"ğŸ¯ High-value customers: {df['target_high_value'].sum()} ({df['target_high_value'].mean()*100:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create synthetic retail dataset for demonstration\n",
        "print(\"ğŸ“¦ Creating synthetic retail dataset...\")\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(RANDOM_STATE)\n",
        "n_samples = 10000\n",
        "\n",
        "# Create retail-specific features\n",
        "data = {\n",
        "    'customer_age': np.random.randint(18, 80, n_samples),\n",
        "    'annual_income': np.random.lognormal(10, 0.5, n_samples),\n",
        "    'purchase_frequency': np.random.poisson(12, n_samples),\n",
        "    'avg_basket_size': np.random.exponential(50, n_samples),\n",
        "    'days_since_last_purchase': np.random.exponential(30, n_samples),\n",
        "    'loyalty_score': np.random.beta(2, 5, n_samples) * 100,\n",
        "    'product_category_electronics': np.random.binomial(1, 0.3, n_samples),\n",
        "    'product_category_clothing': np.random.binomial(1, 0.4, n_samples),\n",
        "    'product_category_food': np.random.binomial(1, 0.5, n_samples),\n",
        "    'seasonal_factor': np.sin(np.random.uniform(0, 2*np.pi, n_samples)) + 1,\n",
        "    'marketing_channel_online': np.random.binomial(1, 0.6, n_samples),\n",
        "    'marketing_channel_email': np.random.binomial(1, 0.4, n_samples),\n",
        "    'customer_segment': np.random.choice(['Premium', 'Standard', 'Budget'], n_samples, p=[0.2, 0.5, 0.3]),\n",
        "    'region': np.random.choice(['North', 'South', 'East', 'West'], n_samples),\n",
        "    'is_weekend': np.random.binomial(1, 0.3, n_samples)\n",
        "}\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Create target variable (sales amount) with realistic relationships\n",
        "target_sales = (\n",
        "    df['annual_income'] * 0.001 +\n",
        "    df['purchase_frequency'] * 5 +\n",
        "    df['avg_basket_size'] * 2 +\n",
        "    df['loyalty_score'] * 1.5 +\n",
        "    df['product_category_electronics'] * 50 +\n",
        "    df['seasonal_factor'] * 20 +\n",
        "    np.random.normal(0, 50, n_samples)  # Add noise\n",
        ")\n",
        "\n",
        "# Ensure positive values\n",
        "df['target_sales'] = np.maximum(target_sales, 0)\n",
        "\n",
        "# Create binary classification target\n",
        "df['target_high_value'] = (df['target_sales'] > df['target_sales'].quantile(0.7)).astype(int)\n",
        "\n",
        "print(f\"âœ… Dataset created with {len(df)} samples and {len(df.columns)} features\")\n",
        "print(f\"ğŸ“Š Target variable (sales) range: ${df['target_sales'].min():.2f} - ${df['target_sales'].max():.2f}\")\n",
        "print(f\"ğŸ¯ High-value customers: {df['target_high_value'].sum()} ({df['target_high_value'].mean()*100:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Feature Engineering Workflow\n",
        "\n",
        "**ğŸ“Š Category**: ML Workflow\n",
        "\n",
        "**ğŸ‘¤ Author**: Data Science Team\n",
        "\n",
        "**ğŸ“… Created**: 2024-01-15\n",
        "\n",
        "**ğŸ”„ Last Updated**: 2024-01-15\n",
        "\n",
        "**â±ï¸ Estimated Runtime**: 15-20 minutes\n",
        "\n",
        "**ğŸ¯ Purpose**: Demonstrate advanced feature engineering techniques including automated feature selection, transformation, and preprocessing for machine learning workflows.\n",
        "\n",
        "**ğŸ“‹ Prerequisites**: \n",
        "- Basic understanding of machine learning concepts\n",
        "- Familiarity with pandas and scikit-learn\n",
        "- Understanding of feature engineering principles\n",
        "\n",
        "**ğŸ“Š Datasets Used**:\n",
        "- Synthetic retail dataset: Generated dataset with sales, product, and customer features\n",
        "- Size: 10,000 rows, 15 features\n",
        "- Target: Sales prediction (regression) or customer category (classification)\n",
        "\n",
        "**ğŸ”§ Tools & Libraries**:\n",
        "- pandas: Data manipulation and analysis\n",
        "- scikit-learn: Machine learning algorithms and preprocessing\n",
        "- matplotlib/seaborn: Data visualization\n",
        "- numpy: Numerical computations\n",
        "- Custom ml_utils: Advanced ML utilities\n",
        "\n",
        "**ğŸ“ˆ Key Outcomes**:\n",
        "- Automated feature selection and engineering\n",
        "- Comprehensive preprocessing pipeline\n",
        "- Feature importance analysis\n",
        "- Optimized feature set for model training\n",
        "\n",
        "**ğŸ”— Related Notebooks**:\n",
        "- [02-model-training.ipynb](02-model-training.ipynb): Model training with engineered features\n",
        "- [03-model-evaluation.ipynb](03-model-evaluation.ipynb): Model evaluation and validation\n",
        "- [data-exploration-template.ipynb](../templates/data-exploration-template.ipynb): Data exploration template\n",
        "\n",
        "**ğŸ“ Change Log**:\n",
        "- v1.0.0 (2024-01-15): Initial implementation with automated feature engineering\n",
        "- v1.0.1 (2024-01-15): Added polynomial features and interaction terms\n",
        "- v1.0.2 (2024-01-15): Enhanced feature selection methods\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“š Table of Contents\n",
        "\n",
        "1. [Environment Setup](#environment-setup)\n",
        "2. [Data Loading and Exploration](#data-loading-exploration)\n",
        "3. [Feature Preprocessing](#feature-preprocessing)\n",
        "4. [Automated Feature Selection](#automated-feature-selection)\n",
        "5. [Feature Engineering](#feature-engineering)\n",
        "6. [Feature Validation](#feature-validation)\n",
        "7. [Pipeline Creation](#pipeline-creation)\n",
        "8. [Results Summary](#results-summary)\n",
        "9. [Next Steps](#next-steps)\n",
        "\n",
        "---\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
